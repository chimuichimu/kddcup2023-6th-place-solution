{"cells":[{"cell_type":"markdown","metadata":{"id":"v--J7LUqL9Cj"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33306,"status":"ok","timestamp":1686074041124,"user":{"displayName":"ichimu chimu","userId":"14422138505087900635"},"user_tz":-540},"id":"BnKnD-UQWSEg","outputId":"df6a583a-e0dd-4883-c44e-cdf2596d5cda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4128,"status":"ok","timestamp":1686074045240,"user":{"displayName":"ichimu chimu","userId":"14422138505087900635"},"user_tz":-540},"id":"8Af_47xxWVhC","outputId":"e83ce77e-288e-466c-e3f4-7284b4d00a46"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3)\n","Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0)\n"]}],"source":["!pip install polars"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27471,"status":"ok","timestamp":1686074072663,"user":{"displayName":"ichimu chimu","userId":"14422138505087900635"},"user_tz":-540},"id":"ou8Cq4Rx5zhJ","outputId":"954bac9d-37ba-43b1-c23c-241d8466bd55"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gensim==4.0.1\n","  Downloading gensim-4.0.1.tar.gz (23.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.22.4)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.10.1)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (6.3.0)\n","Building wheels for collected packages: gensim\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n","Failed to build gensim\n","\u001b[31mERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install gensim==4.0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-RBDJdwWdI-"},"outputs":[],"source":["import os\n","import gc\n","import math\n","import random\n","from collections import defaultdict, Counter\n","from typing import List, Dict\n","import joblib\n","import pickle\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from numba import njit\n","import polars as pl\n","import pandas as pd\n","import lightgbm as lgb\n","from sklearn.model_selection import GroupKFold\n","from gensim.models import Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"hEPw4yzayxut"},"source":["## constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3OCFFMJZoNy"},"outputs":[],"source":["EXP_NAME = \"exp115\"\n","DIR = \"/gdrive/MyDrive/amazon_kdd_2023/\"\n","K_FOLDS = 3\n","SEED = 42\n","LOCALES = [\"UK\", \"JP\", \"DE\"]\n","MAKE_TRAIN = True\n","MAKE_TEST = False\n","\n","# This parameter controls to which end item the candidate is tied.\n","# For example, if [1,2], candidates are generated from the last item and second last item in each session.\n","LAST_NS = [1, 2, 3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a48esfu9yCnO"},"outputs":[],"source":["USE_FEATURES = [\n","    # === candidate features ===\n","    \"co_visit_weight_last1\", \"consective_1_weight_last1\", \"consective_3_weight_last1\", \"consective_5_weight_last1\", \"similarity_score_last1\", \"bert_distance_last1\", \"lift_last1\", \"prone_distance_last1\",\n","    \"co_visit_weight_last2\", \"consective_1_weight_last2\", \"consective_3_weight_last2\", \"consective_5_weight_last2\", \"similarity_score_last2\", \"bert_distance_last2\", \"lift_last2\", \"prone_distance_last2\",\n","    \"co_visit_weight_last3\", \"consective_1_weight_last3\", \"consective_3_weight_last3\", \"consective_5_weight_last3\", \"similarity_score_last3\", \"bert_distance_last3\", \"lift_last3\", \"prone_distance_last3\",\n","    \"imf_score\", \"bpr_score\",\n","    \"co_visit_rank_last1\", \"consective_1_rank_last1\", \"consective_3_rank_last1\", \"consective_5_rank_last1\", \"similarity_rank_last1\", \"bert_rank_last1\", \"lift_rank_last1\", \"prone_rank_last1\",\n","    \"co_visit_rank_last2\", \"consective_1_rank_last2\", \"consective_3_rank_last2\", \"consective_5_rank_last2\", \"similarity_rank_last2\", \"bert_rank_last2\", \"lift_rank_last2\", \"prone_rank_last2\",\n","    \"co_visit_rank_last3\", \"consective_1_rank_last3\", \"consective_3_rank_last3\", \"consective_5_rank_last3\", \"similarity_rank_last3\", \"bert_rank_last3\", \"lift_rank_last3\", \"prone_rank_last3\",\n","    \"imf_rank\", \"bpr_rank\",\n","    # === session features ===\n","    \"S_session_length\",\n","    \"S_nunique_brand\",\n","    \"S_ratio_unique_brand\",\n","    \"S_nunique_item\",\n","    \"S_ratio_repurchase\",\n","    \"S_locale\",\n","    \"S_mean_price\", \"S_max_price\", \"S_min_price\", \"S_std_price\", \"S_total_amount\",\n","    \"S_color_not_null_count\", \"S_size_not_null_count\", \"S_model_not_null_count\", \"S_material_not_null_count\", \"S_author_not_null_count\",\n","    \"S_last_item_price\",\n","    # === product features ===\n","    \"P_price\",\n","    \"P_purchase_count\", \"P_purchase_count_global\",\n","    \"P_total_amount\",\n","    \"P_brand_purchase_count\", \"P_brand_purchase_count_global\",\n","    \"P_brand_mean_price\", \"P_brand_max_price\", \"P_brand_min_price\", \"P_brand_std_price\", \"P_total_brand_amount\",\n","    \"P_price_diff_to_avg_brand_price\",\n","    \"P_n_unique_locale\",\n","    \"P_is_color_null\", \"P_is_size_null\", \"P_is_model_null\", \"P_is_material_null\", \"P_is_author_null\",\n","    \"P_purchase_count_ratio_to_locale\", \"P_purchase_amount_ratio_to_locale\", \"P_purchase_count_ratio_to_brand\", \"P_purchase_amount_ratio_to_brand\",\n","    # === session * product features ===\n","    \"SP_price_diff_to_mean_price\", \"SP_price_diff_to_min_price\", \"SP_price_diff_to_max_price\", \"SP_price_diff_to_last_price\",\n","    \"SP_brand_price_diff_to_mean_price\", \"SP_brand_price_diff_to_min_price\", \"SP_brand_price_diff_to_max_price\", \"SP_brand_price_diff_to_last_price\",\n","    \"SP_same_brand_last1\", \"SP_same_brand_last2\", \"SP_same_brand_last3\",\n","    \"SP_same_color_last1\", \"SP_same_color_last2\", \"SP_same_color_last3\",\n","    \"SP_same_size_last1\", \"SP_same_size_last2\", \"SP_same_size_last3\",\n","    \"SP_same_model_last1\", \"SP_same_model_last2\", \"SP_same_model_last3\",\n","    \"SP_same_material_last1\", \"SP_same_material_last2\", \"SP_same_material_last3\",\n","    \"SP_same_author_last1\", \"SP_same_author_last2\", \"SP_same_author_last3\",\n","    \"SP_same_brand_sum\", \"SP_same_color_sum\", \"SP_same_size_sum\", \"SP_same_model_sum\", \"SP_same_material_sum\", \"SP_same_author_sum\",\n","    # === similality features ===\n","    \"imf_similarity\", \"bpr_similarity\",\n","    \"graph_emb_similarity_last1\", \"graph_emb_similarity_last2\", \"graph_emb_similarity_last3\",\n","    \"i2v_similarity_last1\", \"i2v_similarity_last2\", \"i2v_similarity_last3\",\n","]"]},{"cell_type":"markdown","metadata":{"id":"f19D1NtjyBUk"},"source":["## load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrSTmUlut54k"},"outputs":[],"source":["class CandidateMatrix:\n","    def __init__(self, matrix: pl.DataFrame, feat_name: List[str], join_key: str):\n","        self.matrix = matrix\n","        self.feat_name = feat_name\n","        self.join_key = join_key"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYt-dX3dt-A9"},"outputs":[],"source":["train = pl.read_parquet(DIR + \"data/preprocessed/task1/train_task1.parquet\")\n","test = pl.read_parquet(DIR + \"data/preprocessed/task1/test_task1_phase2.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbUyVOQYuJZs"},"outputs":[],"source":["session_feat = pl.read_parquet(DIR + \"data/interim/features/task1/session_feature_06.parquet\")\n","product_feat_train = pl.read_parquet(DIR + \"data/interim/features/task1/product_feature_train_08.parquet\")\n","product_feat_test = pl.read_parquet(DIR + \"data/interim/features/task1/product_feature_test_08.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dCzdlzat_9W"},"outputs":[],"source":["similar_products1 = pl.read_parquet(DIR + \"data/interim/candidates/task1/similar_products_13.parquet\")\n","similar_products2 = pl.read_parquet(DIR + \"data/interim/candidates/task1/similar_products_19.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_HyzlOft23G"},"outputs":[],"source":["if MAKE_TRAIN:\n","    imf_candidates_train = pl.read_parquet(DIR + \"data/interim/candidates/task1/imf_15_for_train_or_eval.parquet\")\n","    bpr_candidates_train = pl.read_parquet(DIR + \"data/interim/candidates/task1/bpr_01_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_1 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_25_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_2 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_30_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_3 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_29_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_4 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_27_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_5 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_28_for_train_or_eval.parquet\")\n","    prone_matrix_train = pl.read_parquet(DIR + \"data/interim/candidates/task1/prone_03_for_local_or_eval.parquet\")\n","\n","    candidate_matrices_train =[\n","        CandidateMatrix(co_visit_matrix_train_1, [\"co_visit_weight\", \"co_visit_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_2, [\"lift\", \"lift_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_3, [\"consective_1_weight\", \"consective_1_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_4, [\"consective_3_weight\", \"consective_3_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_5, [\"consective_5_weight\", \"consective_5_rank\"], \"item\"),\n","        CandidateMatrix(similar_products1, [\"similarity_score\", \"similarity_rank\"], \"item\"),\n","        CandidateMatrix(similar_products2, [\"bert_distance\", \"bert_rank\"], \"item\"),\n","        CandidateMatrix(bpr_candidates_train, [\"bpr_score\", \"bpr_rank\"], \"session\"),\n","        CandidateMatrix(imf_candidates_train, [\"imf_score\", \"imf_rank\"], \"session\"),\n","        CandidateMatrix(prone_matrix_train, [\"prone_distance\", \"prone_rank\"], \"item\"),\n","    ]\n","\n","    # item2vec model\n","    i2v_models_train = {}\n","    for locale in LOCALES:\n","        i2v_models_train[locale] = Word2Vec.load(DIR + f\"models/task1/item2vec_{locale}_05_for_train_or_eval.model\")\n","\n","    # imf\n","    imf_model_train = {}\n","    user_id2index_train = {}\n","    item_id2index_train = {}\n","    for locale in LOCALES:\n","        imf_model_train[locale] = np.load(DIR + f\"models/task1/imf_15_{locale}_model_for_train_or_eval.npz\")\n","        with open(DIR + f\"models/task1/imf_15_{locale}_user_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n","            user_id2index_train[locale] = pickle.load(f)\n","        with open(DIR + f\"models/task1/imf_15_{locale}_item_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n","            item_id2index_train[locale] = pickle.load(f)\n","\n","    # bpr\n","    bpr_model_train = {}\n","    bpr_user_id2index_train = {}\n","    bpr_item_id2index_train = {}\n","    for locale in LOCALES:\n","        bpr_model_train[locale] = np.load(DIR + f\"models/task1/bpr_01_{locale}_model_for_train_or_eval.npz\")\n","        with open(DIR + f\"models/task1/bpr_01_{locale}_user_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n","            bpr_user_id2index_train[locale] = pickle.load(f)\n","        with open(DIR + f\"models/task1/bpr_01_{locale}_item_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n","            bpr_user_id2index_train[locale] = pickle.load(f)\n","\n","    # prone\n","    graph_embs_train = {}\n","    item_id2indices_prone_train = {}\n","    for locale in LOCALES:\n","        graph_embs_train[locale] = np.load(DIR + f\"models/task1/graph_embedding_03_{locale}_for_local_train_or_eval.npy\")\n","        with open(DIR + \"data/interim/graph/task1/graph_\" + f\"item_id2index_03_{locale}_for_train_or_eval.pickle\", \"rb\") as f:\n","            item_id2indices_prone_train[locale] = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nj76vGat25L"},"outputs":[],"source":["if MAKE_TEST:\n","    imf_candidates_test = pl.read_parquet(DIR + \"data/interim/candidates/task1/imf_15_for_inference.parquet\")\n","    bpr_candidates_test = pl.read_parquet(DIR + \"data/interim/candidates/task1/bpr_01_for_inference.parquet\")\n","    co_visit_matrix_test_1 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_25_for_inference.parquet\")\n","    co_visit_matrix_test_2 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_30_for_inference.parquet\")\n","    co_visit_matrix_test_3 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_29_for_inference.parquet\")\n","    co_visit_matrix_test_4 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_27_for_inference.parquet\")\n","    co_visit_matrix_test_5 = pl.read_parquet(DIR + \"data/interim/candidates/task1/co_visit_matrix_28_for_inference.parquet\")\n","    prone_matrix_test = pl.read_parquet(DIR + \"data/interim/candidates/task1/prone_03_for_inference.parquet\")\n","\n","    candidate_matrices_test =[\n","        CandidateMatrix(co_visit_matrix_test_1, [\"co_visit_weight\", \"co_visit_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_2, [\"lift\", \"lift_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_3, [\"consective_1_weight\", \"consective_1_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_4, [\"consective_3_weight\", \"consective_3_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_5, [\"consective_5_weight\", \"consective_5_rank\"], \"item\"),\n","        CandidateMatrix(similar_products1, [\"similarity_score\", \"similarity_rank\"], \"item\"),\n","        CandidateMatrix(similar_products2, [\"bert_distance\", \"bert_rank\"], \"item\"),\n","        CandidateMatrix(imf_candidates_test, [\"imf_score\", \"imf_rank\"], \"session\"),\n","        CandidateMatrix(bpr_candidates_test, [\"bpr_score\", \"bpr_rank\"], \"session\"),\n","        CandidateMatrix(prone_matrix_test, [\"prone_distance\", \"prone_rank\"], \"item\"),\n","    ]\n","\n","    # item2vec\n","    i2v_models_test = {}\n","    for locale in LOCALES:\n","        i2v_models_test[locale] = Word2Vec.load(DIR + f\"models/task1/item2vec_{locale}_05_for_inference.model\")\n","\n","    # imf model\n","    imf_model_test = {}\n","    user_id2index_test = {}\n","    item_id2index_test = {}\n","    for locale in LOCALES:\n","        imf_model_test[locale] = np.load(DIR + f\"models/task1/imf_15_{locale}_model_for_inference.npz\")\n","        with open(DIR + f\"models/task1/imf_15_{locale}_user_id2index_for_inference.pickle\", \"rb\") as f:\n","            user_id2index_test[locale] = pickle.load(f)\n","        with open(DIR + f\"models/task1/imf_15_{locale}_item_id2index_for_inference.pickle\", \"rb\") as f:\n","            item_id2index_test[locale] = pickle.load(f)\n","\n","    # imf model\n","    bpr_model_test = {}\n","    bpr_user_id2index_test = {}\n","    bpr_item_id2index_test = {}\n","    for locale in LOCALES:\n","        bpr_model_test[locale] = np.load(DIR + f\"models/task1/bpr_01_{locale}_model_for_inference.npz\")\n","        with open(DIR + f\"models/task1/bpr_01_{locale}_user_id2index_for_inference.pickle\", \"rb\") as f:\n","            bpr_user_id2index_test[locale] = pickle.load(f)\n","        with open(DIR + f\"models/task1/bpr_01_{locale}_item_id2index_for_inference.pickle\", \"rb\") as f:\n","            bpr_item_id2index_test[locale] = pickle.load(f)\n","\n","    # prone\n","    graph_embs_test = {}\n","    item_id2indices_prone_test = {}\n","    for locale in LOCALES:\n","        graph_embs_test[locale] = np.load(DIR + f\"models/task1/graph_embedding_03_{locale}_for_inference.npy\")\n","        with open(DIR + \"data/interim/graph/task1/graph_\" + f\"item_id2index_03_{locale}_for_inference.pickle\", \"rb\") as f:\n","            item_id2indices_prone_test[locale] = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"E-NbXCWfyDZt"},"source":["## functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D75acjHUyEyt"},"outputs":[],"source":["# functions for data processing\n","def generate_candidates(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -> pl.DataFrame:\n","\n","    def add_last_n_item(df: pl.DataFrame, last_n: int) -> pl.DataFrame:\n","        last_item_list = []\n","        prev_items_list = df[\"prev_items\"].to_list()\n","        for prev_items in prev_items_list:\n","            try:\n","                last_item_list.append(prev_items[-last_n])\n","            except IndexError:\n","                last_item_list.append(None)\n","        df = df.with_columns(pl.Series(name=f\"last_item_{last_n}\", values=last_item_list))\n","        return df\n","\n","    # add last_item columns\n","    for last_n in LAST_NS:\n","        df = add_last_n_item(df, last_n)\n","\n","    # generate candidates\n","    candidates = []\n","\n","    # candidates tied to items\n","    for last_n in LAST_NS:\n","        for candidate_matrix in candidate_matrices:\n","            if candidate_matrix.join_key == \"item\":\n","                # join candidates to last_n item\n","                candidate = df.join(candidate_matrix.matrix, left_on=[f\"last_item_{last_n}\", \"locale\"], right_on=[\"item\", \"locale\"], how=\"left\")\n","                candidate = candidate.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\"))) # remove already purchased items\n","\n","                # keep candidates for feature addition later\n","                original_feat_names = candidate_matrix.feat_name\n","                feat_names = [f\"{x}_last{last_n}\" for x in original_feat_names]\n","                tmp = candidate[[\"session_id\", \"candidate_item\"] + original_feat_names]\n","                for original_feat_name, feat_name in zip(original_feat_names, feat_names):\n","                    tmp = tmp.rename({original_feat_name:feat_name})\n","                candidates.append(tmp)\n","\n","    # candidates tied to session\n","    for candidate_matrix in candidate_matrices:\n","        if candidate_matrix.join_key == \"session\":\n","            # join candidates to session\n","            candidate = df.join(candidate_matrix.matrix, on=\"session_id\", how=\"left\")\n","            candidate = candidate.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\"))) # remove already purchased items\n","\n","            # keep candidates for feature addition later\n","            candidates.append(candidate[[\"session_id\", \"candidate_item\"] + candidate_matrix.feat_name])\n","\n","    cand_all = pl.concat([df[[\"session_id\", \"candidate_item\"]] for df in candidates])\n","\n","    # remove duplicate candidates\n","    cand_all = cand_all.unique(subset=[\"session_id\", \"candidate_item\"])\n","\n","    # join candidates\n","    df = df.join(cand_all, on=[\"session_id\"], how=\"left\")\n","\n","    # add features derived from the candidate\n","    for candidate in candidates:\n","        df = df.join(candidate, on=[\"session_id\", \"candidate_item\"], how=\"left\")\n","\n","    return df\n","\n","\n","def add_label(df: pl.DataFrame) -> pl.DataFrame:\n","    df = df.with_columns((pl.col(\"candidate_item\") == pl.col(\"next_item\")).cast(pl.Int8).alias(\"label\"))\n","    return df\n","\n","def filter_null(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -> pl.DataFrame:\n","    feat_names = []\n","    for candidate_matrix in candidate_matrices:\n","        if candidate_matrix.join_key == \"item\":\n","            for last_n in LAST_NS:\n","                for feat_name in candidate_matrix.feat_name:\n","                    feat_names.append(f\"{feat_name}_last{last_n}\")\n","        elif candidate_matrix.join_key == \"session\":\n","            feat_names.extend(candidate_matrix.feat_name)\n","    df = df.filter(\n","        ~pl.all(pl.col(feat_names).is_null())\n","    )\n","    return df\n","\n","def negative_sample(df: pl.DataFrame) -> pl.DataFrame:\n","    negatives = df.filter(df[\"label\"] == 0)\n","    negatives = negatives.sample(fraction=0.1, seed=SEED)\n","    df = pl.concat([df.filter(df[\"label\"] > 0), negatives])\n","    return df\n","\n","def filter_session_not_include_positive(df: pl.DataFrame) -> pl.DataFrame:\n","    positive_sessions = df.filter(pl.col(\"label\")==1)[\"session_id\"].to_list()\n","    df = df.filter(df[\"session_id\"].is_in(positive_sessions))\n","    return df\n","\n","def add_features(\n","    df: pl.DataFrame,\n","    session_feat_df:pl.DataFrame, product_feat_df:pl.DataFrame,\n","    i2v_models:Dict[str, Word2Vec],\n","    imf_model, user_id2index, item_id2index,\n","    bpr_model, bpr_user_id2index, bpr_item_id2index,\n","    graph_embs, item_id2indices_prone) -> pl.DataFrame:\n","\n","    @njit()\n","    def calc_cos_sim(v1, v2):\n","        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","\n","    # session features\n","    df = df.join(session_feat_df, on=\"session_id\", how=\"left\")\n","\n","    # product features\n","    df = df.join(product_feat_df, left_on=[\"candidate_item\", \"locale\"], right_on=[\"id\", \"locale\"], how=\"left\")\n","\n","    # session * product features\n","    df = df.with_columns([\n","        (pl.col(\"P_price\") - pl.col(\"S_mean_price\")).alias(\"SP_price_diff_to_mean_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_min_price\")).alias(\"SP_price_diff_to_min_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_max_price\")).alias(\"SP_price_diff_to_max_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_last_item_price\")).alias(\"SP_price_diff_to_last_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_mean_price\")).alias(\"SP_brand_price_diff_to_mean_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_min_price\")).alias(\"SP_brand_price_diff_to_min_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_max_price\")).alias(\"SP_brand_price_diff_to_max_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_last_item_price\")).alias(\"SP_brand_price_diff_to_last_price\"),\n","    ])\n","\n","    for last_n in LAST_NS:\n","        df = df.with_columns([\n","            ((pl.col(\"P_brand\") == pl.col(f\"S_brand_last{last_n}\"))&(pl.col(f\"S_brand_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_brand_last{last_n}\"),\n","            ((pl.col(\"P_color\") == pl.col(f\"S_color_last{last_n}\"))&(pl.col(f\"S_color_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_color_last{last_n}\"),\n","            ((pl.col(\"P_size\") == pl.col(f\"S_size_last{last_n}\"))&(pl.col(f\"S_size_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_size_last{last_n}\"),\n","            ((pl.col(\"P_model\") == pl.col(f\"S_model_last{last_n}\"))&(pl.col(f\"S_model_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_model_last{last_n}\"),\n","            ((pl.col(\"P_material\") == pl.col(f\"S_material_last{last_n}\"))&(pl.col(f\"S_material_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_material_last{last_n}\"),\n","            ((pl.col(\"P_author\") == pl.col(f\"S_author_last{last_n}\"))&(pl.col(f\"S_author_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_author_last{last_n}\"),\n","        ])\n","    df = df.with_columns([\n","        (pl.col(\"SP_same_brand_last1\") + pl.col(\"SP_same_brand_last2\") + pl.col(\"SP_same_brand_last3\")).cast(pl.UInt8).alias(\"SP_same_brand_sum\"),\n","        (pl.col(\"SP_same_color_last1\") + pl.col(\"SP_same_color_last2\") + pl.col(\"SP_same_color_last3\")).cast(pl.UInt8).alias(\"SP_same_color_sum\"),\n","        (pl.col(\"SP_same_size_last1\") + pl.col(\"SP_same_size_last2\") + pl.col(\"SP_same_size_last3\")).cast(pl.UInt8).alias(\"SP_same_size_sum\"),\n","        (pl.col(\"SP_same_model_last1\") + pl.col(\"SP_same_model_last2\") + pl.col(\"SP_same_model_last3\")).cast(pl.UInt8).alias(\"SP_same_model_sum\"),\n","        (pl.col(\"SP_same_material_last1\") + pl.col(\"SP_same_material_last2\") + pl.col(\"SP_same_material_last3\")).cast(pl.UInt8).alias(\"SP_same_material_sum\"),\n","        (pl.col(\"SP_same_author_last1\") + pl.col(\"SP_same_author_last2\") + pl.col(\"SP_same_author_last3\")).cast(pl.UInt8).alias(\"SP_same_author_sum\"),\n","    ])\n","\n","    # imf similarity between last items and candidates\n","    dfs = []\n","    for locale in list(df[\"locale\"].unique()):\n","        df_by_locale = df.filter(pl.col(\"locale\") == locale)\n","\n","        sessions = df_by_locale[\"session_id\"].to_list()\n","        candidates = df_by_locale[\"candidate_item\"].to_list()\n","        user_index2vector = dict(enumerate(imf_model[locale][\"user_factors\"]))\n","        item_index2vector = dict(enumerate(imf_model[locale][\"item_factors\"]))\n","        imf_similarities = []\n","        for session, candidate in zip(sessions, candidates):\n","            try:\n","                user_index, item_index = user_id2index[locale][session], item_id2index[locale][candidate]\n","                v1, v2 = user_index2vector[user_index], item_index2vector[item_index]\n","                sim = calc_cos_sim(v1, v2)\n","            except (KeyError, TypeError): # KeyError if the item is not in the imf training data. TypeError if there are no candidates in a session.\n","                sim = 0\n","            imf_similarities.append(np.float32(sim))\n","        df_by_locale = df_by_locale.with_columns(pl.Series(name=\"imf_similarity\", values=imf_similarities).cast(pl.Float32))\n","        dfs.append(df_by_locale)\n","    df = pl.concat(dfs)\n","\n","\n","    # bpr similarity between last items and candidates\n","    dfs = []\n","    for locale in list(df[\"locale\"].unique()):\n","        df_by_locale = df.filter(pl.col(\"locale\") == locale)\n","\n","        sessions = df_by_locale[\"session_id\"].to_list()\n","        candidates = df_by_locale[\"candidate_item\"].to_list()\n","        user_index2vector = dict(enumerate(bpr_model[locale][\"user_factors\"]))\n","        item_index2vector = dict(enumerate(bpr_model[locale][\"item_factors\"]))\n","        bpr_similarities = []\n","        for session, candidate in zip(sessions, candidates):\n","            try:\n","                user_index, item_index = bpr_user_id2index[locale][session], bpr_item_id2index[locale][candidate]\n","                v1, v2 = user_index2vector[user_index], item_index2vector[item_index]\n","                sim = calc_cos_sim(v1, v2)\n","            except (KeyError, TypeError): # KeyError if the item is not in the imf training data. TypeError if there are no candidates in a session.\n","                sim = 0\n","            bpr_similarities.append(np.float32(sim))\n","        df_by_locale = df_by_locale.with_columns(pl.Series(name=\"bpr_similarity\", values=bpr_similarities).cast(pl.Float32))\n","        dfs.append(df_by_locale)\n","    df = pl.concat(dfs)\n","\n","    for last_n in LAST_NS:\n","        # item2vec similarity between last items and candidates\n","        dfs = []\n","        for locale in LOCALES:\n","            df_by_locale = df.filter(pl.col(\"locale\") == locale)\n","\n","            last_items = df_by_locale[f\"last_item_{last_n}\"].to_list()\n","            cand_items = df_by_locale[\"candidate_item\"].to_list()\n","            item_similalities = []\n","            for last_item, cand_item in zip(last_items, cand_items):\n","                try:\n","                    sim = i2v_models[locale].wv.similarity(last_item, cand_item)\n","                except (KeyError, TypeError): # KeyError if the item is not in the item2vec training data. TypeError if there are no candidates in a session.\n","                    sim = -1\n","                item_similalities.append(np.float32(sim))\n","            df_by_locale = df_by_locale.with_columns(pl.Series(name=f\"i2v_similarity_last{last_n}\", values=item_similalities).cast(pl.Float32))\n","            dfs.append(df_by_locale)\n","        df = pl.concat(dfs)\n","\n","        # prone similarity between last items and candidates\n","        dfs = []\n","        for locale in LOCALES:\n","            df_by_locale = df.filter(pl.col(\"locale\") == locale)\n","            last_items = df_by_locale[f\"last_item_{last_n}\"].to_list()\n","            cand_items = df_by_locale[\"candidate_item\"].to_list()\n","            item_similalities = []\n","            item_index2vector = dict(enumerate(graph_embs[locale]))\n","            for last_item, cand_item in zip(last_items, cand_items):\n","                try:\n","                    item_index1 = item_id2indices_prone[locale][last_item]\n","                    item_index2 = item_id2indices_prone[locale][cand_item]\n","                    v1, v2 = item_index2vector[item_index1], item_index2vector[item_index2]\n","                    sim = calc_cos_sim(v1, v2)\n","                except (KeyError, TypeError): # KeyError if the item is not in the item2vec training data. TypeError if there are no candidates in a session.\n","                    sim = -1\n","                item_similalities.append(np.float32(sim))\n","            df_by_locale = df_by_locale.with_columns(pl.Series(name=f\"graph_emb_similarity_last{last_n}\", values=item_similalities).cast(pl.Float32))\n","            dfs.append(df_by_locale)\n","        df = pl.concat(dfs)\n","    return df\n","\n","def fill_null_and_cast(df: pl.DataFrame) -> pl.DataFrame:\n","    df = df.with_columns([\n","        pl.col(\"co_visit_weight_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last1\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"bert_distance_last1\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"similarity_score_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"co_visit_weight_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last2\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"bert_distance_last2\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"similarity_score_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"co_visit_weight_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last3\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"bert_distance_last3\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"similarity_score_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"imf_score\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"bpr_score\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"co_visit_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"bert_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"co_visit_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"bert_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"co_visit_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"bert_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"bpr_rank\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"S_locale\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"S_session_length\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_nunique_item\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_nunique_brand\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_color_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_size_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_model_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_material_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_author_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_ratio_unique_brand\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_ratio_repurchase\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_mean_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_max_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_min_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_std_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_last_item_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_total_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_count\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_purchase_count_global\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_n_unique_locale\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_color_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_size_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_model_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_material_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_author_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_brand_purchase_count\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_brand_purchase_count_global\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_total_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_locale_purchase_count\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_total_locale_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_count_ratio_to_locale\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_amount_ratio_to_locale\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_count_ratio_to_brand\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_amount_ratio_to_brand\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_brand_mean_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_brand_max_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_brand_min_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_brand_std_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_total_brand_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_price_diff_to_avg_brand_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_price_diff_to_mean_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_price_diff_to_min_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_price_diff_to_max_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_price_diff_to_last_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_brand_price_diff_to_mean_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_brand_price_diff_to_min_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_brand_price_diff_to_max_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_brand_price_diff_to_last_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_same_brand_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_brand_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_brand_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_color_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_color_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_color_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_size_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_size_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_size_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_model_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_model_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_model_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_material_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_material_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_material_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_author_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_author_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_author_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_brand_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_color_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_size_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_model_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_material_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_author_sum\").fill_null(0).cast(pl.UInt8),\n","\n","    ])\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"_FPA8z96Zil9"},"source":["## fix seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEckSTT1ZkDG"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","seed_everything(SEED)"]},{"cell_type":"markdown","metadata":{"id":"9d-OJvwkk8SN"},"source":["# Process data"]},{"cell_type":"markdown","metadata":{"id":"CgmlzAJyRXpD"},"source":["## train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15952444,"status":"ok","timestamp":1686108310642,"user":{"displayName":"ichimu chimu","userId":"14422138505087900635"},"user_tz":-540},"id":"4izMBylaa3Lq","outputId":"3284c9fa-0e96-4b5e-cf8a-b02b90537a87"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/66 [00:00<?, ?it/s]<ipython-input-13-c8475332f611>:102: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 1d, A), array(float64, 1d, A))\n","  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","100%|██████████| 66/66 [9:25:42<00:00, 514.28s/it]\n"]}],"source":["if MAKE_TRAIN:\n","    n_rows = 50_000\n","    for idx, df in tqdm(enumerate(train.iter_slices(n_rows=n_rows)), total=math.ceil(train.height/n_rows)): # specify \"total\" parameter to display tqdm progress bar\n","        df = generate_candidates(df, candidate_matrices_train)\n","        df = df.drop(\"prev_items\")\n","        df = add_label(df)\n","        df = filter_null(df, candidate_matrices_train)\n","        df = filter_session_not_include_positive(df)\n","        df = negative_sample(df)\n","        df = add_features(df, session_feat, product_feat_train, i2v_models_train, imf_model_train, user_id2index_train, item_id2index_train, bpr_model_train, bpr_user_id2index_train, bpr_item_id2index_train, graph_embs_train, item_id2indices_prone_train)\n","        df = fill_null_and_cast(df)\n","        df = df[[\"session_id\", \"candidate_item\", \"label\"] + USE_FEATURES]\n","        df.write_parquet(DIR + f\"data/interim/for_ranker/task1/train_chunk_{EXP_NAME}_{idx}.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVIxzJGA0bXf"},"outputs":[],"source":["if MAKE_TEST:\n","    n_rows = 15_000\n","    for idx, df in tqdm(enumerate(test.iter_slices(n_rows=n_rows)), total=math.ceil(test.height/n_rows)): # specify \"total\" parameter to display tqdm progress bar\n","        # process data\n","        df = generate_candidates(df, candidate_matrices_test)\n","        df = df.drop(\"prev_items\")\n","        df = add_features(df, session_feat, product_feat_test, i2v_models_test, imf_model_test, user_id2index_test, item_id2index_test, bpr_model_test, bpr_user_id2index_test, bpr_item_id2index_test, graph_embs_test, item_id2indices_prone_test)\n","        df = fill_null_and_cast(df)\n","        df.write_parquet(DIR + f\"data/interim/for_ranker/task2/test_chunk_{EXP_NAME}_{idx}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"py39","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.1 (default, Dec 11 2020, 09:29:25) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"7bf9227335684a71c61bfe5034b029174b8b8625eabcf049c0e38a0e70d1b74d"}}},"nbformat":4,"nbformat_minor":0}
